{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Common NLTK tasks\n",
    "\n",
    "In this session we provide an quick introduction to the field of *corpus linguistics*. We then engage with common uses of NLTK within these areas, such as sentence segmentation, tokenisation and stemming. Often, NLTK has inbuilt methods for performing these tasks. As a learning exercise, however, we will sometimes build basic tools from scratch.\n",
    "\n",
    "## Corpus linguistics\n",
    "\n",
    "Though corpus linguistics has been around since the 1950s, it is only in the last 20 years that its methods have been made available to individual researchers. GUIs including [Wordsmith Tools](http://www.lexically.net/wordsmith/) and [AntConc](http://www.laurenceanthony.net/software.html).\n",
    "\n",
    "Alongside the development of GUIs, there has also been a shift from *general, balanced corpora* (corpora seeking to represent a language generally) toward *specialised corpora* (corpora containing texts of one specific type, from one speaker, etc.). More and more commonly, texts are taken from the Web.\n",
    "\n",
    "> We'll discuss building corpora from online texts in a bit more detail later in the course.\n",
    "\n",
    "After a long period of resistance, corpus linguistics has gained acceptence within a number of research areas. A few popular applications are within:\n",
    "\n",
    "* **Lexicography** (creating usage-based definitions of words and locating real examples)\n",
    "* **Language pedagogy** (advanced language learners can use a concordancing GUI or collocation tests to understand how certain words are used in the target language)\n",
    "* **Discourse analysis** (researching how meaning is made beyond the level of the clause/sentence)\n",
    "\n",
    "Notably, corpus linguistic methods have been embraced within the emerging paradigm of Digital Humanities, where it's sometimes called *distant reading*.\n",
    "\n",
    "### Corpora and discourse\n",
    "\n",
    "As hardware, software and data become more and more available, people have started using corpus linguistic methods for discourse-analytic work. Paul Baker refers the combination of corpus linguistics and (critical) discourse analysis as a [*useful methodological synergy*](#ref:baker). Corpora bring objectivity and empiricism to a qualitative, interpretative tradition, while discourse- analytic methods provide corpus linguistics with a means of contextualising abstracted results.\n",
    "\n",
    "Within this area, researchers rely on corpora to varying extents. In *corpus-driven* discourse analysis, researchers interpret the corpus based on the findings of the corpus interrogation. In *corpus-assisted* discourse analysis, researchers may use corpora to provide evidence about the way a given person/idea/discourse is commonly represented by certain people/in certain publications etc.\n",
    "\n",
    "Our work here falls under the *corpus-driven* heading, as we are exploring the dataset without any major hypotheses in mind.\n",
    "\n",
    "> Some linguists remain skeptical of corpus linguistics generally. In\n",
    "a well-known critique, Henry Widdowson ([2000, p. 6-7](#ref:widdowson)) said:\n",
    ">\n",
    "> Corpus linguistics \\[...\\] (there) is no doubt that this is an immensely important development in descriptive linguistics. That is not the issue here. The quantitative analysis of text by computer reveals facts about actual language behaviour which are not, or at least not immediately, accessible to intuition. There are frequencies of occurrence of words, and regular patterns of collocational co-occurrence, which users are unaware of, though they must be part of their competence in a procedural sense since they would not otherwise be attested. They are third person observed data ('When do they use the word X?') which are different from the first person data of introspection ('When do I use the word X?'), and the second person data of elicitation ('When do you use the word X?'). Corpus analysis reveals textual facts, fascinating profiles of produced language, and its concordances are always springing surprises. They do indeed reveal a reality about language usage which was hitherto not evident to its users.\n",
    ">\n",
    "> But this achievement of corpus analysis at the same time necessarily defines its limitations. For one thing, since what is revealed is contrary to intuition, then it cannot represent the reality of first person awareness. We get third person facts of what people do, but not the facts of what people know, nor what they think they do: they come from the perspective of the observer looking on, not the introspective of the insider. In ethnomethodogical terms, we do not get member categories of description. Furthermore, it can only be one aspect of what they do that is captured by such quantitative analysis. For, obviously enough, the computer can only cope with the material products ofwhat people do when they use language. It can only analyse the textual traces of the processes whereby meaning is achieved: it cannot account for the complex interplay of linguistic and contextual factors whereby discourse is enacted. It cannot produce ethnographic descriptions of language use. In reference to Hymes's components of communicative competence (Hymes 1972), we can say that corpus analysis deals with the textually attested, but not with the encoded possible, nor the contextually appropriate.\n",
    ">\n",
    "> To point out these rather obvious limitations is not to undervalue corpus analysis but to define more clearly where its value lies. What it can do is reveal the properties of text, and that is impressive enough. But it is necessarily only a partial account of real language. For there are certain aspects of linguistic reality that it cannot reveal at all. In this respect, the linguistics of the attested is just as partial as the linguistics of the possible.\n",
    "\n",
    "## Loading a corpus\n",
    "\n",
    "First, we have to load a corpus. We'll use a text file containing posts to [an Australian online forum](http://www.ozpolitic.com/forum/YaBB.pl?board=global) for discussing politics. It's full of very interesting natural language data!\n",
    "\n",
    "This file is available online, at the Research Platforms [GitHub](https://github.com/resbaz/nltk). We can ask Python to get it for us.\n",
    "\n",
    "> Later in the course, we'll discuss how to extract data from the Web and turn this data into a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # a library for working with urls \n",
    "url = 'http://git.io/v47HI'\n",
    "# url = \"https://raw.githubusercontent.com/resbaz/nltk/master/corpora/oz_politics/ozpol.txt\" # define the url\n",
    "response = requests.get(url, verify=False)\n",
    "raw_text = response.text\n",
    "raw_text = raw_text.lower() # make it lowercase, to keep things simple\n",
    "len(raw_text) # how many characters does it contain?\n",
    "raw_text[:2000] # first 2000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the corpus to our cloud as a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('forum.txt', 'w') as fo:\n",
    "    fo.write(raw_text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('forum.txt')\n",
    "loaded_raw_text = f.read()\n",
    "loaded_raw_text = unicode(loaded_raw_text.lower(), 'utf-8') # make it lowercase and unicode\n",
    "print(len(loaded_raw_text))\n",
    "print(loaded_raw_text[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation\n",
    "\n",
    "We can now start to turn our corpus into a structured resource. At present, we have `raw_text`, a very, very long string of text.\n",
    "\n",
    "We should break the string into segments. First, we'll split the corpus into sentences. This task is a pretty boring one, and it's tough for us to improve on existing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sents = sent_tokenizer.tokenize(raw_text)\n",
    "sents[101:111]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we have sentences. Now what?\n",
    "\n",
    "## Tokenisation\n",
    "\n",
    "Tokenisation is simply the process of breaking texts down into words. We already did a little bit of this in Session 1. We won't build our own tokenizer, because it's not much fun. NLTK has one we can rely on.\n",
    "\n",
    "Keep in mind that definitions of tokens are not standardised, especially for languages other than English. Serious problems arise when comparing two corpora that have been tokenised differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sents = [nltk.word_tokenize(i) for i in sents]\n",
    "tokenized_sents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have is a list of lists: sentences and their tokens. We might also want to flatten the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = sum(tokenized_sents, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the sentence segmentation is still important, however, as it helped with proper handling of sentence final punctuation, for example.\n",
    "\n",
    "Let's jump quickly into analysing the corpus. Like in the last lesson, we might want to count tokens. That can be very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def howmany(word):\n",
    "    return sum([s.count(word) for s in tokenized_sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. Two problems, though. First, this takes one word of interest at a time. Second, we can only search for literal words.\n",
    "\n",
    "## Loops\n",
    "\n",
    "A common programming method for reperforming some function is the *loop*. Most prototypical is the *for loop*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how would we put this to work with our `howmany()` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = ['terror', 'refugee', 'refugees', 'islam']\n",
    "for word in wordlist:\n",
    "    print word, howmany(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powerful, eh? The next problem, however, is that we're stuck writing out 'refugee' and `refugees`.\n",
    "\n",
    "## Regular expressions\n",
    "\n",
    "Regular expressions are a language for searching strings of characters. For us right now, they're a language inside a language. Alphanumeric characters and some punctuation work just like normal searches, but some special characters have different meanings. You've probably already seen some of these in the wild, like the asterisk as wildcard.\n",
    "\n",
    "At their simplest, we can use `re.search()` to search \n",
    "\n",
    "re.search(r'[a-z]+ing\\b', raw_text)\n",
    "\n",
    "Let's use regular expressions to search our non-segmented text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall('muslims?', raw_text)\n",
    "re.findall('.*muslim.*', raw_text)\n",
    "re.findall('[^\\s]+ muslims? [^\\s]+', raw_text)\n",
    "Counter(re.findall('([^\\s]+) muslims? [^\\s]+', raw_text)).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this kind of approach to start matching nouns and their plurals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'([a-z]+(es|s))', raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could use it to stem our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r'([a-z]+)(ing|s|ed|er)([^a-z])')\n",
    "re.sub(regex, r'\\1\\3', raw_text)\n",
    "\n",
    "Can you update `howmany()` to handle a regular expression? Use our `flat` corpus if it's easier.\n",
    "\n",
    "```python\n",
    "def howmany(regex):\n",
    "    return len([re.findall(regex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are limits to this kind of approach, though. What are they?\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Stemming is the task of finding the stem of a word. So, *cats --> cat*, or *taking --> take*. It is an important task when counting words, as often the counting each inflection seperately is not particuarly helpful: forms of the verb 'to be' might seem under-represented if we could *is, are, were, was, am, be, being, been* separately.\n",
    "\n",
    "NLTK has pre-programmed stemmers, but we can build our own using some of the skills we've already learned.\n",
    "\n",
    "A stemmer is the kind of thing that would make a good function, so let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']: # list of suffixes\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)] # delete the suffix\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give it a word to stem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem('friends')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it over some text and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in tokenized_sents[:5]:\n",
    "    for token in sent:\n",
    "        print(stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output, we can see that the stemmer works: *wingers* becomes *winger*, and *tearing* becomes *tear*. But, sometimes it does things we don't want: *Nothing* becomes *noth*, and *mate* becomes *mat*.\n",
    "\n",
    "We can see that this approach has obvious limitations. So, let's rely on a purpose-built stemmer. These rely in part on dictionaries. Note the subtle differences between the two possible stemmers.\n",
    "\n",
    "Currently, we have a list of sentences, and each sentence is a list of words. We need to flatten this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for sent in tokenized_sents:\n",
    "    for token in sent:\n",
    "        tokens.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try our NLTK's stemmers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate stemmers\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "# stem each word in tokens\n",
    "stems = [lancaster.stem(t) for t in tokens]  # replace lancaster with porter here\n",
    "print(stems[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both stemmers handle some things rather poorly. The main reason for this is that they are not aware of the *word class* of any particular word: *nothing* is a noun, and nouns ending in *ing* should not have *ing* removed by the stemmer (swing, bling, ring...). Later in the course, we'll start annotating corpora with grammatical information. This improves the accuracy of stemmers a lot.\n",
    "\n",
    "> Note: stemming is not *always* the best thing to do: though *thing* is the stem of *things*, things has a unique meaning, as in *things will improve*. If we are interested in vague language, we may not want to collapse things --> thing.\n",
    "\n",
    "## Collocation\n",
    "\n",
    "> *You shall know a word by the company it keeps.* - J.R. Firth, 1957\n",
    "\n",
    "Collocation is a very common area of interest in corpus linguistics. Words pattern together in both expected and unexpected ways. In some contexts, *drug* and *medication* are synonymous, but it would be very rare to hear about *illicit* or *street medication*. Similarly, doctors are unlikely to prescribe the *correct* or *appropriate drug*.\n",
    "\n",
    "This kind of information may be useful to lexicographers, discourse analysts, or advanced language learners.\n",
    "\n",
    "In NLTK, collocation works from ordered lists of tokens. We made this earlier as `tokens`, didn't we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not, here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for sent in tokenized_sents:\n",
    "    for token in sent:\n",
    "        tokens.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's feed these to an NLTK function for measuring collocations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the functions needed for collocation work\n",
    "from nltk.collocations import *\n",
    "# define statistical tests for bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# go and find bigrams\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "# measure which bigrams are important and print the top 30\n",
    "print(sorted(finder.nbest(bigram_measures.raw_freq, 30)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, that tells us a little: we can see that terrorists, Muslims and the Middle\n",
    "East are commonly collocating in the text. At present, we are only looking for\n",
    "immediately adjacent words. So, let's expand out search to a window of *five\n",
    "words either side*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''window size'' specifies the distance at which \n",
    "# two tokens can still be considered collocates\n",
    "finder = BigramCollocationFinder.from_words(tokens, window_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the appearance of very common words! Let's use NLTK's stopwords list\n",
    "to remove entries containing these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "finder.apply_word_filter(lambda w: w.lower() in ignored_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There! Now we have some interesting collocates. Finally, let's remove\n",
    "punctuation-only entries, or entries that are *n't*, as this is caused by\n",
    "different tokenisers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder.apply_word_filter(lambda w: w.lower() in ignored_words or not w.isalnum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a lot more info on collocation at the [NLTK homepage](http://www.nltk.org/howto/collocations.html).\n",
    "\n",
    "Completed bigrams code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the functions needed for collocation work\n",
    "from nltk.collocations import *\n",
    "# define statistical tests for bigrams\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# go and find bigrams\n",
    "finder = BigramCollocationFinder.from_words(tokens, window_size=5)\n",
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "finder.apply_word_filter(lambda w: w.lower() in ignored_words or not w.isalnum())\n",
    "# measure which bigrams are important and print the top 30\n",
    "result = sorted(finder.nbest(bigram_measures.raw_freq, 30))\n",
    "for bigram in result:\n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering/n-grams\n",
    "\n",
    "Clustering is the task of finding words that are commonly **immediately** adjacent (as opposed to collocates, which may just be nearby). This is also often called n-grams: bigrams are two tokens that appear together, trigrams are three, etc.\n",
    "\n",
    "Clusters/n-grams have a spooky ability to tell us what a text is about.\n",
    "\n",
    "There's also a method for n-gram production in NLTK. We can use this to understand how n-gramming works.\n",
    "\n",
    "Below, we get lists of any ten adjacent tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "# define a sentence\n",
    "sentence = 'give a man a fish and you feed him for a day; teach a man to fish and you feed him for a lifetime'  \n",
    "tokenised = nltk.word_tokenize(sentence)\n",
    "# length of ngram\n",
    "n = 10\n",
    "tengrams = list(ngrams(tokenised, n))\n",
    "print(tengrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are plenty of tengrams in there! What we're interested in, however, is\n",
    "duplicated n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(text, gramsize = 3, threshold = 4):\n",
    "    \"\"\"get ngrams of gramsize size and threshold minimum occurrences\"\"\"\n",
    "    if type(text) != list:\n",
    "        text = nltk.word_tokenize(text)\n",
    "    # skip punctuation?\n",
    "    # \n",
    "    ngms = ngrams(text, gramsize)\n",
    "    cntr = Counter(ngms)\n",
    "    return Counter({k: v for k, v in cntr.items() if v >= threshold})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's defined, let's run it, looking for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammer(raw, gramsize = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops, punctutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add me:\n",
    "text = [token for token in text if token.isalnum()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many results? Let's set a higher threshold than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrammer(raw, gramsize = 3, threshold = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordancing with regular expressions\n",
    "\n",
    "We've already done a bit of concordancing. In discourse-analytic research, concordancing is often used to perform thematic categorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)  # formats our tokens for concordancing\n",
    "text.concordance(\"muslims\")\n",
    "\n",
    "# A problem with the NLTK concordancer is that it only works with individual tokens. What if we want to find words that end with **ment*, or words beginning with *poli**?\n",
    "\n",
    "# We already searched text with Regular Expressions. It's not much more work to build regex functionality into our own concordancer.\n",
    "\n",
    "# From running the code below, you can see that bracketting sections of our regex causes results to split into lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a regex for different aussie words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it's ugly, but it works. We can see five bracketted results, each containing three strings. The first and third strings are the left-context and right-context. The second of the three strings is the search term.\n",
    "\n",
    "These three sections are, with a bit of tweaking, the same as the output given by a concordancer.\n",
    "\n",
    "Let's go ahead and turn our regex seacher into a concordancer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordancer(text, query):\n",
    "    for line in text.splitlines():\n",
    "        if query in line:\n",
    "            start, end = line.split(query, 1)  \n",
    "            concline = [start[-30:], query, end[:30]]\n",
    "            print(\"\\t\".join(concline).expandtabs(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordancer(raw, 'australia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! With six lines of code, we've officially created a function that improves on the one provided by NLTK! And think how easy it would be to add more functionality: an argument dictating the size of the window (currently 30 characters), or printing line numbers beside matches, would be pretty easy to add, as well.\n",
    "\n",
    "> Adding too much functionality is known as *feature creep*. It's often best to keep your functions simple and more varied. An old adage in programming is to *make each program do one thing well*.\n",
    "\n",
    "In the cells below, try concordancing a few things. Also try creating variables with concordance results, and then manipulate the lists. If you encounter problems with the way the concordancer runs, alter the function and redefine it. If you want, try implementing the window size variable!\n",
    "\n",
    "> If you wanted to get really creative, you could try stemming concordance or n-gram results!\n",
    "\n",
    "## Summary\n",
    "\n",
    "That's the end of session three! Great work.\n",
    "\n",
    "So, some of these tasks are a little dry---seeing results as lists of words and scores isn't always a lot of fun. But ultimately, they're pretty important things to know if you want to avoid the 'black box approach', where you simply dump words into a machine and analyse what the machine spits out.\n",
    "\n",
    "Remember that almost every task in corpus linguistics/distance reading depends on how we segment our data into sentences, clauses, words, etc.\n",
    "\n",
    "Building a stemmer from scratch taught us how to use regular expressions, and their power. But, we also saw that they weren't perfect for the task. In later lessons, we'll use more advanced methods to normalise our data.\n",
    "\n",
    "# Bibliography\n",
    "\n",
    "<a id=\"ref:baker\"></a>\n",
    "Baker, P., Gabrielatos, C., Khosravinik, M., Krzyzanowski, M., McEnery, T., &\n",
    "Wodak, R. (2008). A useful methodological synergy? Combining critical discourse\n",
    "analysis and corpus linguistics to examine discourses of refugees and asylum\n",
    "seekers in the UK press. Discourse & Society, 19(3), 273-306.\n",
    "\n",
    "<a id=\"firth\"></a>\n",
    "Firth, J. (1957).  *A Synopsis of Linguistic Theory 1930-1955*. In: Studies in\n",
    "Linguistic Analysis, Philological Society, Oxford; reprinted in Palmer, F. (ed.)\n",
    "1968 Selected Papers of J. R. Firth, Longman, Harlow.\n",
    "\n",
    "<a id=\"ref:hymes\"></a>\n",
    "Hymes, D. (1972). On communicative competence. In J. Pride & J. Holmes (Eds.),\n",
    "Sociolinguistics (pp. 269-293). Harmondsworth: Penguin Books. Retrieved from [ht\n",
    "tp://humanidades.uprrp.edu/smjeg/reserva/Estudios%20Hispanicos/espa3246/Prof%20S\n",
    "unny%20Cabrera/ESPA%203246%20-%20On%20Communicative%20Competence%20p%2053-73.pdf\n",
    "](http://humanidades.uprrp.edu/smjeg/reserva/Estudios%20Hispanicos/espa3246/Prof\n",
    "%20Sunny%20Cabrera/ESPA%203246%20-%20On%20Communicative%20Competence%20p%2053-73\n",
    ".pdf)\n",
    "\n",
    "<a id=\"ref:widdowson\"></a>\n",
    "Widdowson, H. G. (2000). On the limitations of linguistics applied. Applied\n",
    "Linguistics, 21(1), 3. Available at [http://applij.oxfordjournals.org/content/21\n",
    "/1/3.short](http://applij.oxfordjournals.org/content/21/1/3.short)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
